\chapter{Calculation Packages}

Broadwick supports several simulation and fitting models. In this chapter we will give an outline of these methods and how they can be simulated (and combing) using the Broadwick framework. We will, for the most part, dispense with the theory and concentrate on how the methods are implemented in Broadwick. The code snippets in this chapter are taken from the examples that are distributed with the Broadwick source code.



\section{Markov Chains}\index{Markov Chains}

A Markov chain is a random sequence of states where the current state depends solely on the previous state. In this sense, it is a ``memoryless'' process as the transition from one state to the next does not depend on the sequence of states that preceeded it.

A Markov Chain can be implemented in Broadwick using the MonteCarloStep and MarkovChain classes. The MonteCarloStep encapsulates the functionality of a state by maintaining a collection of the coordinates defining the state as a java.util.Map<String,Double> (i.e. the name and value of the state). A MarkovChain object is constructed using a MonteCarloStep object as an initial point and, optionally, a MarkovProposalFunction for generating the next step. The generateNextStep method uses the proposal function to generate the next step in the chain as the following code snippet demonstrates,

\begin{lstlisting}

final Map<String, Double> coordinates = new LinkedHashMap<>();
        {
            coordinates.put("x", 0.0);
            coordinates.put("y", 0.0);
        }
final MonteCarloStep initialStep = new MonteCarloStep(coordinates);

final MarkovChain mc = new MarkovChain(initialStep);
for (int i = 0; i < chainLength; i++) {
    final MonteCarloStep nextStep = mc.generateNextStep(mc.getCurrentStep());
    mc.setCurrentStep(nextStep);

    log.trace("{}", nextStep.toString());
}
\end{lstlisting}

By default, a MarkovNormalProposal object is used to generate the next step by sampling from a Normal distribution centered on the current coordinate and with a standard deviation of 1. New proposal classes implement the MarkovProposalFunction interface and using this object when creating the Markov Chain.

\begin{lstlisting}
    final MarkovProposalFunction myProposer = new MarkovProposalFunction();
    final MarkovChain mc = new MarkovChain(initialStep, myProposer);
\end{lstlisting}


\section{Monte Carlo Simulation}\index{Monte Carlo Simulation}
Monte Carlo simulation is a broad class of methods that reply on repeated simulation of [random] processes to derive numerical results. Broadwick uses the MonteCarloScenario abstract class to encapsulate the process to be simulated, which is used by the MonteCarlo class to implement the Monte Carlo process.

Internally, the MonteCarlo class uses a producer-comsumer pattern by creating a ThreadFactory to spawn several simulation processes which are in turn consumed by a MonteCarloResults object. This MonteCarloResults object takes the results of each simulation (the producer threads) and calculates the required statistics.

These classes (MonteCarloScenario, MonteCarloResults) are extended for each implementation. By way of example, we will calculate $\pi$ by throwing darts randomly at a square dartboard and calaulating the fraction that fall within the unit circle encompasses by the square.

First, we implement the run() method of our class that extends the MonteCarloScenario class (this class has an internal random number generator, rng, to generate random number).
\begin{lstlisting}
    @Override
    public MonteCarloResults run() {
        final MyResultsConsumer results = new MyResultsConsumer();

        final double x = rng.getDouble(-1, 1);
        final double y = rng.getDouble(-1, 1);

        final double r = Math.sqrt(x * x + y * y);
        if (r < 1) {
            results.addHit();
        } else {
            results.addMiss();
        }
        return results;
    }
\end{lstlisting}

The MonteCarloResults class is responsible for both storing the results of each simulation and for acting as a consumer
object that maintains a collection of the results returned by the producers.

\begin{lstlisting}
    class MyResultsConsumer implements MonteCarloResults {

    @Override
    public double getExpectedValue() {
        return hits.getSum() / (hits.getSum() + misses.getSum());
    }

    @Override
    public Samples getSamples() {
        return hits;
    }

    @Override
    public String toCsv() {
        return String.format("\%d ;  \%d", hits.getSize(), misses.getSize());
    }

    @Override
    public MonteCarloResults join(final MonteCarloResults results) {
        // This is where the results of the producers are dealt with.
        final MyResultsConsumer r = (MyResultsConsumer) results;
        this.hits.add(r.hits);
        this.misses.add(r.misses);

        return this;
    }

    public void addHit() {
        hits.add(1);
    }

    public void addMiss() {
        misses.add(1);
    }
    
    @Override
    public void reset() {
    }
    
    @Getter
    private final Samples hits = new Samples();
    @Getter
    private final Samples misses = new Samples();
}
\end{lstlisting}

These two classes are utilised thus
\begin{lstlisting}
    MonteCarlo mc = new MonteCarlo(new Simulation(), 1000);
    mc.setResultsConsumer(new MyResultsConsumer());
    mc.run();

    final MyResultsConsumer results = (MyResultsConsumer) mc.getResults();
    log.info("Hits : Misses = {}", results.toCsv());
    log.info("Estimation of Pi = {}", 4 * results.getExpectedValue());
\end{lstlisting}

\subsection{Markov Chain Monte Carlo}\index{Markov Chain Monte Carlo}
Markov chains can be combined with Monte Carlo simulation to explore a parameter space by using the Markov chain to `walk' through the parameter space while Monte Carlo simulation is to determine the state of the system and each step in the walk. Thus, Markov chain Monte Carlo (MCMC) methods can be used to sample from a probability distribution by using the Markov chain (coupled with a rejection function to accept or reject propsed steps) to find the desired distribution. 

A MarkovChainMonteCarlo object will create a Markov chain and run the Monte Carlo simulation at each step using the Metropolis-Hastings algorithm (the MetropolisHastings class) to accept successive steps based on the results of the Monte Carlo simultaion at the given step.

A MarkovChainObserver object can be added to the MarkovChainMonteCarlo object which will be informed when a step has been completed. This observer can be used to save the results of the simulation.

By default a Metropolis-Hastings, but it is easy to implement an alternative (in the following example we will use the Metropolis algorithm with a log-likelihood).

\begin{lstlisting}
    final MonteCarloStep step = new MonteCarloStep(<initial step>);
    final MarkovChainMonteCarlo mcmc = new MarkovChainMonteCarlo(
                    myModel, <numScenarios>,
                    myMonteCarloScenarioResults,
                    myMarkovStepGenerator);


    mcmc.setAcceptor(new MonteCarloAcceptor() {
                @Override
                public boolean accept(final MonteCarloResults oldResult, final MonteCarloResults newResult) {
                    final double ratio = newResult.getExpectedValue() - oldResult.getExpectedValue();
                    return Math.log(generator.getDouble()) < ratio / smoothingRatio;
                }
            });

    MyMarkovChainObserver myMcmcObserver = new MyMarkovChainObserver(mcmc);
    mcmc.addObserver(myMcmcObserver);

    mcmc.run();
\end{lstlisting}


\section{Approximate Bayesian Computation (ABC)}\index{Approximate Bayesian Computation (ABC)}

\section{Ordinary Differential Equations (ODEs)}\index{Ordinary Differential Equations (ODEs)}



